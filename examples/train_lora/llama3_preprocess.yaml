### model
model_name_or_path: /kaggle/input/llama-3.2/transformers/3b-instruct/1

### method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

### dataset
dataset: identity,alpaca_zh_demo
template: llama3
cutoff_len: 1024
max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 16
tokenized_path: saves/Llama-3.2-3B-Instruct/dataset/sft

### output
output_dir: saves/Llama-3.2-3B-Instruct/lora/sft
overwrite_output_dir: true
